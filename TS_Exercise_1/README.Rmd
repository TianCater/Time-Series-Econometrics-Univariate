---
title: "README"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


The purpose of this assignment is to find the best univariate, linear, stochastic model (BULSM) for two given time series using the Box-Jenkins methodology. {Spread} refers to the time series spread between a 1 year and a 3 month US Treasury bill, and {y} a simulated stationary ARMA(p,q) process. 

The analysis for each series will be structured as follows:

1. Present basic first step analysis. (Plot, ACF, PACF with interpretations and conclusions)

2. Investigate the following models for potential BULSM: AR(1), AR(2), MA(2), ARMA(1,1), ARMA(2,1). This includes selecting the best model according to the AIC    & BIC measures.

3. Evaluate whether the selected model is adequate. That is, whether the selected model is both:
  
3.1. Congruent. 
  

Requiress the residuals to be white noise and normally distributed. Autocorrelation test (Ljung Box test), ARCH test (ARCH LM test), and normality (Jarque-Bera test).
  
3.2. Parsimonious 
       
The minimum requirement is that the estimates be statistically significant. Then select the fitted model with the lowest AIC and BIC values, this model is the then       the most parsimonious.

4. Conclude with the estimated results of the BULSM.



#Importing and structuring the data
```{r Data Loading and Cleaning, echo=FALSE}


library(readxl)
DF1 <- read_excel("data/TSexercise1data.xlsx")

#load required packages

pacman::p_load(dplyr,stats,fixest, tidyverse, huxtable, hrbrthemes, modelsummary, glue,forecast, FinTS)

#Isolate columns as numeric vectors/variables

Spread <- DF1 %>% dplyr::select(spread)
y <- DF1%>% dplyr::select(y)



#VITAL, mutate dates to :Type Date

DF1 <- DF1 %>%
   group_by(dates) %>%
   mutate(dates=as.Date(dates, format = "%Y.%m.%d"))

Date <- DF1 %>% dplyr::select(dates)


```



```{r generating the variables as time series, echo=FALSE}


TS_spread <- ts(Spread,start=c(1984,1,1),end = c(2001,8,1),frequency = 12)

TS_y <-  ts(y,start=c(1984,1,1),end = c(2001,8,1),frequency = 12)


```



---------{y}-----------

1. Present basic first step analysis. (Plot, ACF, PACF with interpretations and conclusions)

```{r Plot ACF and PACF}

##Plot,ACF and PACF for {y}

forecast::tsdisplay(TS_y)

```


2. Investigate the following models for potential BULSM: AR(1), AR(2), MA(2), ARMA(1,1), ARMA(2,1). This includes selecting the best model according to the AIC & BIC measures.


```{r }
##{y}


###AR(1)
AR1_y <- forecast::Arima(TS_y,c(1,0,0))
plot(TS_y)
lines(fitted(AR1_y), col='blue')

###summary
summary(AR1_y)

```


```{r }
###AR(2)
AR2_y <- forecast::Arima(TS_y,c(2,0,0))
plot(TS_y)
lines(fitted(AR2_y), col='blue')

###summary
summary(AR2_y)


```


```{r}
###MA(2)

MA2_y <- forecast::Arima(TS_y,c(0,0,2))
plot(TS_y)
lines(fitted(MA2_y), col='blue')


###summary
summary(MA2_y)
```


```{r }
###ARMA(1,1)

ARMA11_y <- forecast::Arima(TS_y,c(1,0,1))
plot(TS_y)
lines(fitted(ARMA11_y),col='blue')

###summary
summary(ARMA11_y)
```


```{r }
###ARMA(2,1)

ARMA21_y <- forecast::Arima(TS_y,c(2,0,1))
plot(TS_y)
lines(fitted(ARMA21_y),col='blue')

###summary
summary(ARMA21_y)



```


Visually, the majority of the models seem to fit rather well. However, the AR(1) model has the lowest AIC and BIC values (205.09 and 215.16, respectively). Therefore, by this measure we preliminary select this model and conduct further tests for adequacy. 



3. Evaluate whether the selected model is adequate. That is, whether the selected model is both congruent (3.1) and parsimonious (3.2)

  3.1. Congruent 
  
Congruence requires the residuals to be white noise and normally distributed. Test for autocorrelation (Ljung Box test), ARCH test (ARCH LM test), and normality (Jarque-Bera test). 


       
```{r}

AR1_y <- forecast::Arima(TS_y,c(1,0,0))

AR1_y

coeftest(AR1_y)

```


```{r Residual graphics and Ljung-Box test for AR1}

Res2 <- forecast::checkresiduals(AR1_y, lag=20)


```
       
       
Visually, The corelogram (ACF) of the residuals does indeed look like a realization of white noise. However, we interpret the Ljung-Box test for 20 lags to make more informed conclusions.       
       
  3.1.1. Ljung Box Test

The Ljung Box test is a way to test for the absence of serial autocorrelation, up to a specified lag k. Therefore the bigger the lag the stronger the test. When the Ljung Box test is applied to the residuals of an ARIMA model, the degrees of freedom  must be equal to m-b, where where m is the time lag, and b the number of estimated parameters in the ARIMA(p,q) model. Thankfully, R selects the correct df by default.

The test determines whether or not errors are white noise or whether there is something more behind them; whether or not the autocorrelations for the errors or residuals are non zero. The underlying hypothesis test is given by:

            H0= No serial correlation between residuals (independence)
            H1= Serial correlation between residuals (dependence)       


The p-value = 0.373 > 0.05 means we fail to reject the null. Therefore the residuals are independent at the 95% level and thus an AR(1) model provides a good model fit (by the measure of autocorrelation between residuals).

  3.1.2. ARCH LM test (Heteroskedasticity)
  
```{r ARCH LM test fir AR1}

pacman::p_load(tseries)

pacman::p_load(FinTS)

ArchTest(residuals(AR1_y))

```
  
 
The ARCH LM test indicates whether the residuals are homoskedastistic. 

         
            H0= No ARCH effects 
            H1= ARCH effects
            
The p-value = 0.5044 > 0.05 implies that the we fail to reject null. Therefore there is no ARCH effects within the residuals. i.e. Homoskedasticity.  (This is an obvious result be construction of the y process)


  3.1.3. Jarque Bera Test 

```{r Jarque Bera test for AR1}

jarque.bera.test(residuals(AR1_y))

```


The Jarque-Bera test is a goodness-of-fit test that determines whether or not sample data have skewness and kurtosis that matches a normal distribution. A normal distribution has a skew of zero (i.e. it’s perfectly symmetrical around the mean) and a kurtosis of three; kurtosis tells you how much data is in the tails and gives you an idea about how “peaked” the distribution is.

Therefore, we are testing against the null hypothesis that the residuals are normally distributed at the 5% significance level: 

            H0= Residuals are normally distributed 
            H1= Residuals are not normally distributed 
            


The p-value = 0.4045 > 0.05 means we fail to reject the null at the 5% significance level. That is, the residuals are normally distributed.

  3.1.3. Conclusion

For the y process, the AR(1) model satisfies the necessary requirements for congruence. 

      
       
 3.2. Parsimonious 
  
The minimum requirement is that the estimates be statistically significant. Then select the fitted model with the lowest AIC and BIC values, this model is then the most parsimonious. All these requirements are met.
       
  3.2.1. Conclusion 
  
By its construction, we know that the AR(1) model will be parsimonious as y is a simulated stationary ARMA(p,q) process. For confirmation however, the                    coefficients are all statistically significant. Additionally (as already indicated in the first step), the AR(1) model has the lowest AIC and BIC values between          the pool of ARIMA models tested. 
  
       
       
       
4. Conclude with the estimated results of the BULSM.














--------{Spread}--------

1. Present basic first step analysis. (Plot, ACF, PACF with interpretations and conclusions)

```{r Plot ACF PACF}

##Plot,ACF and PACF for {Spread}

forecast::tsdisplay(TS_spread)


```


2. Investigate the following models for potential BULSM: AR(1), AR(2), MA(2), ARMA(1,1), ARMA(2,1). This includes selecting the best model according to the AIC & BIC measures.

```{r Investigating and fitting different models for each process}

##{Spread}

###AR(1)

AR1 <- forecast::Arima(TS_spread,c(1,0,0))
plot(TS_spread)
lines(fitted(AR1), col='blue')

###summary
summary(AR1)
```


```{r }
###AR(2)
AR2 <- forecast::Arima(TS_spread,c(2,0,0))
plot(TS_spread)
lines(fitted(AR1), col='blue')

###summary
summary(AR2)
```


```{r }
###MA(2)

MA2 <- forecast::Arima(TS_spread,c(0,0,2))
plot(TS_spread)
lines(fitted(MA2), col='blue')

###summary
summary(MA2)
```


```{r }
###ARMA(1,1)

ARMA11 <- forecast::Arima(TS_spread,c(1,0,1))
plot(TS_spread)
lines(fitted(ARMA11),col='blue')

###summary
summary(ARMA11)
```


```{r }
###ARMA(2,1)

ARMA21 <- forecast::Arima(TS_spread,c(2,0,1))
plot(TS_spread)
lines(fitted(ARMA21),col='blue')

###summary
summary(ARMA21)
```


Visually, it is difficult (or near impossible in this case) to state which ARMA model is the best fit .However, the ARMA(1,1) model has the lowest AIC and BIC values (-277.2 and -263.77, respectively), and by this measure we select the ARMA(1,1) model as the preliminary best fit and conduct further tests to verify the model as the BULSM.


3. Evaluate whether the selected model is adequate. That is, whether the selected model is both congruent (3.1) and parsimonious (3.2)

3.1. Congruent

Congruence requires the residuals to be white noise and normally distributed. Autocorrelation test (Ljung Box test), ARCH test (ARCH LM test), and normality (Jarque-Bera test).
       
```{r}

pacman::p_load(lmtest)

ARMA11 <- forecast::Arima(TS_spread,c(1,0,1))

ARMA11

coeftest(ARMA11)

```

       
```{r Residual graphics and Ljung-Box test for ARMA11}

Res1 <- forecast::checkresiduals(ARMA11, lag = 20)


```
       
Visually, The corelogram (ACF) of the residuals does not particularly look like a realization of white noise. However, we interpret the Ljung-Box test for 20 lags to make informed conclusions.          
       


  3.1.1. Ljung Box Test

The Ljung Box test is a way to test for the absence of serial autocorrelation, up to a specified lag k. Therefore the bigger the lag the stronger the test. When the Ljung Box test is applied to the residuals of an ARIMA model, the degrees of freedom  must be equal to m-b, where where m is the time lag, and b the number of estimated parameters in the ARIMA(p,q) model. Thankfully, R selects the correct df by default.

The test determines whether or not errors are white noise or whether there is something more behind them; whether or not the autocorrelations for the errors or residuals are non zero. The underlying hypothesis test is given by:

            H0= No serial correlation between residuals (independence)
            H1= Serial correlation between residuals (dependence)       


The p-value = 0.1686 > 0.05 means we fail to reject the null. Therefore the residuals are independent at the 95% level and thus an ARMA(1,1) model provides a good model fit (by the measure of autocorrelation between residuals).


```{r ARCH LM Test for ARMA11}

pacman::p_load(tseries)

ArchTest(residuals(ARMA11))

```



  3.1.2. ARCH LM test (Heteroskedasticity)
 
The ARCH LM tests whether the residuals are homoskedastistic. 

         
            H0= No ARCH effects 
            H1= ARCH effects
            
The p-value = 0.02025 < 0.05 implies that the null is rejected. Therefore there is ARCH effects within the residuals. i.e. Heteroskedasticity.  (Therefore, we already know that the residuals are not normally distributed)


```{r Jarque Bera Test for ARMA11}

jarque.bera.test(residuals(ARMA11))


```


  3.1.3. Jarque Bera Test 

The Jarque-Bera test is a goodness-of-fit test that determines whether or not sample data have skewness and kurtosis that matches a normal distribution. A normal distribution has a skew of zero (i.e. it’s perfectly symmetrical around the mean) and a kurtosis of three; kurtosis tells you how much data is in the tails and gives you an idea about how “peaked” the distribution is.

Therefore, we are testing against the null hypothesis that the residuals are normally distributed at the 5% significance level: 

            H0= Residuals are normally distributed 
            H1= Residuals are not normally distributed 
            


The p-value = 0.003976 > 0.05 means we reject the null at the 5% significance level. That is, the residuals are not normally distributed.


  3.1.4. Conclusion

 

      
       
 3.2. Parsimonious 
  
The minimum requirement is that the estimates be statistically significant. Then select the fitted model with the lowest AIC and BIC values, this model is the then       the most parsimonious. 

Even though the model is clearly not adequate (not congruent), it does fit the minimum requirement for parsimony of statistically significant estimates, and has the lowest AIC and BIC values between the pool of ARIMA models considered. The correct wording would be to say that, within the pool of ARIMA models considered, the ARMA(1,1) model is the most parsimonious. 
       
  3.2.1. Conclusion 
  

  
  
4. Conclude with the estimated results of the BULSM.



###I will now use the auto.Arima() function. The auto.arima() function in R uses a variation of the Hyndman-Khandakar algorithm (Hyndman & Khandakar, 2008), which combines unit root tests, minimisation of the AICc and MLE to obtain an ARIMA model. 


```{r a second attemp for spread}

AA_spread <- auto.arima(TS_spread, seasonal= TRUE,stationary = TRUE, stepwise = TRUE, approximation = FALSE, allowdrift = TRUE)
AA_spread

```
According to the Hyndman-Khandakar algorithm (Hyndman & Khandakar, 2008), the best model fit (provided the criterion that they are stationary) would be an seasonal ARMA(3,1)(1,1) model. Therefore 

We select the seasonal ARMA(3,1)(1,1) model and accordingly conduct the required tests for adequacy.

```{r}
ARMA31_11 <-  forecast::Arima(TS_spread, order=c(3,0,0), seasonal = c(1,0,1))

coeftest(ARMA31_11)

forecast::checkresiduals(ARMA31_11, lag = 20)

ArchTest(residuals(ARMA31_11))

pacman::p_load(tseries)

jarque.bera.test(residuals(ARMA31_11))
```






